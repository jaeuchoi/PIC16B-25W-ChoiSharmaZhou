# -*- coding: utf-8 -*-
"""Word_Recognition_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rp_erQ7B0-G_W2KiiXQT_awimyJu50J6
"""


import os
from keras import utils
import tensorflow_datasets as tfds
import keras
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg

# Loads dataset
# Resize images
# Divide into multiple batches
# Train model using images and their labels

import os

# Function to parse label file and extract image paths and labels, ignoring missing files
def parse_labels(label_file, image_root='/content/word_dataset_new_mar20/words'):
    image_paths = []
    labels = []

    with open(label_file, 'r') as file:
        for line in file:
            line = line.strip()
            # Skip empty lines and comments
            if not line or line.startswith('#'):
                continue

            parts = line.split()

            # Validate line format (should have at least 9 fields)
            if len(parts) < 9:
                continue

            # Extract filename and label
            filename = parts[0]  # Example: a01-000u-00-00
            label = parts[-1]    # Label is the last element

            # Construct path:
            # Format: words/{first three chars}/{first seven chars}/{filename}.png
            # Example: words/a01/a01-000u/a01-000u-00-00.png

            folder_level_1 = filename[:3]
            folder_level_2 = "-".join(filename.split("-")[:2]) # select up to second part in dashes
            image_filename = f"{filename}.png"

            image_path = os.path.join(image_root, folder_level_1, folder_level_2, image_filename)

            # Check if the file exists, skip if it does not
            if os.path.exists(image_path):
                image_paths.append(image_path)
                labels.append(label)

    return image_paths, labels



import tensorflow as tf


IMAGE_HEIGHT, IMAGE_WIDTH = 64, 300  # Unifying size of images - Why 64, 300?
# Average length of english word is 4.7 characters (Googled)
# Assuming a character is square, it means word width is 4.7 times word height
# 64*4.7 = 300

# maybe split the image into single characters?
# ML models only accept unified dimensions


def load_and_preprocess_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_png(img, channels=1)  # Grayscale
    img = tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH])
    img = img / 255.0
    return img

def encode_label(label):
    label_chars = tf.strings.unicode_split(label, input_encoding='UTF-8')
    label_encoded = char_to_num(label_chars)
    return label_encoded


BATCH_SIZE = 32




# CTC loss function
def ctc_loss(y_true, y_pred):
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64") * tf.ones(shape=(batch_len, 1), dtype="int64")
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64") * tf.ones(shape=(batch_len, 1), dtype="int64")

    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    return loss




def preprocess_single_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_png(img, channels=1)
    img = tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH])
    img = img / 255.0
    img = tf.expand_dims(img, axis=0)  # Add batch dimension
    return img

def decode_ctc(predicted_indices):
    chars = num_to_char(predicted_indices)

    # Convert tensor to string
    decoded_text = tf.strings.reduce_join(chars, axis=-1)
    decoded_text = decoded_text.numpy()[0].decode('utf-8')

    # Remove '[UNK]' and blank characters
    final_text = []
    prev_char = ''
    for char in decoded_text:
        if char not in ['[UNK]', '', ' '] and char != prev_char:  # Ignore blanks
            final_text.append(char)
            prev_char = char

    return ''.join(final_text)



def predict_text(model, image_path):
    img_tensor = preprocess_single_image(image_path)
    prediction = model.predict(img_tensor)

    # Convert to character indices
    predicted_indices = tf.argmax(prediction, axis=-1)

    print("Predicted indices:", predicted_indices.numpy())  # Debug print

    return decode_ctc(predicted_indices)